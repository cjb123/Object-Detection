{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Object Detection </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1. Using an Image Classifier(pretrained) to detect objects using keras and openCV </b>\n",
    "\n",
    "Here we take a Convolutional Neural Network trained for image classification (pre-trained RESNET-50) and utilize `image pyramids`, `sliding windows`, and `non-maxima suppression` to build a basic object detector.Basically we combine traditional computer vision object detection algorithms with deep learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> In Image Classification :</b> Input : Image --> Output : Class Label  <br>We present the input image to our neural network, and we obtain a single class label and a probability associated with the class label prediction.This class label characterizes the contents ( the most dominant and  visible contents) of the image.<br>\n",
    "\n",
    "<b> Object Detection :</b> Along with outputting the class labels i.e the objects present in the image, it also outputs where in the image the objects are with multiple bounding box coordinates.<br>\n",
    "\n",
    "More specifically, it outputs 3 values,including : <br>\n",
    "1. A list of bounding boxes, or the (x, y)-coordinates for each object in an image\n",
    "2. The class label associated with each of the bounding boxes\n",
    "3. The probability/confidence score associated with each bounding box and class label\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 1.1 Importing necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from imutils.object_detection import non_max_suppression\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import imutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 1.2 How deep learning image classifier can be converted into an object detector? </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We utilise the elements of traditional computer vision algorithms to convert our CNN image classifier into an object detector.<br>\n",
    "\n",
    "<b>1.2.1</b> The first element we use is <b>`Image Pyramids`</b>: <br>\n",
    "\n",
    "* An “image pyramid” is a multi-scale representation of an image:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Image_pyramid.png\" style =\"height: 40%;width: 40%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the bottom of the pyramid, we have the original sized image .\n",
    "At each subsequent layer, the image is resized and optionally smoothed.\n",
    "The image is progressively subsampled until some stopping criterion is met( when a minimum size has been reached), and no further subsampling is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b>1.2.2</b>The second element we use is <b>`Sliding Windows`</b>: <br>\n",
    "\n",
    "* A sliding window is a fixed-size rectangle that slides from left-to-right and top-to-bottom within an image:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"sliding_window.gif\" style =\"height: 40%;width: 40%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each stop of the window we would:\n",
    "1. Extract the Image within the sliding window\n",
    "2. Input Image to an Image Classifier\n",
    "3. obtain predictions(class label and probability scores)\n",
    "\n",
    "<div style =\"color:green\">Image pyramids and sliding windows helps us localize objects at different locations and multiple scales of the input image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1.2.3</b> The Third element we use is <b>`Non-Maxima Suppression`</b>: <br>\n",
    "\n",
    "The object detectors generally outputs multiple, overlapping bounding boxes surrounding an object in an image.<br>\n",
    "This happens because as the sliding window approaches an image, the classifier outputs larger and larger probabilities of  the object class(i.e higher probability of object being detected) .<br>\n",
    "\n",
    "Since there’s only one object of a particular class,multiple bounding boxes can create a problem.<br>\n",
    "The solution is to apply non-maxima suppression (NMS), which removes weak, overlapping bounding boxes by giving us the ones with higher confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nms.jpg\" style =\"height: 50%;width: 50%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the utility functions for implementing Image Pyramids and Sliding Windows and some other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_pyramid(image, scale=1.5, minSize=(224, 224)):\n",
    "    # yield the original image\n",
    "    yield image\n",
    "    # keep looping over the image pyramid\n",
    "    while True:\n",
    "        w = int(image.shape[1] / scale)\n",
    "        image = imutils.resize(image, width=w)\n",
    "        if image.shape[0] < minSize[1] or image.shape[1] < minSize[0]:\n",
    "              break\n",
    "        yield image\n",
    "        \n",
    "def sliding_window(image, step, ws):\n",
    "      for y in range(0, image.shape[0] - ws[1], step):\n",
    "            for x in range(0, image.shape[1] - ws[0], step):\n",
    "                 yield (x, y, image[y:y + ws[1], x:x + ws[0]])\n",
    "                    \n",
    "def load_images(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 1.3 The steps we follow in the object detection algorithm:</b>\n",
    "\n",
    "1. We input image.\n",
    "2. We construct image pyramid fot the input image.\n",
    "3. For each scale of the image pyramid, we run a sliding window:\n",
    "     3.1. For each stop of the sliding window,we extract the image inside the sliding window(ROI)\n",
    "     3.2. We take the sampled image and pass it through our CNN originally trained for image classification\n",
    "     3.3. Examine the probability of the top class label of the CNN, and if meets a minimum confidence,we record \n",
    "            3.3.1. class label and\n",
    "            3.3.2  location of the sliding window\n",
    "4. We apply non-maxima suppression to the bounding boxes for different classes.\n",
    "5. We return the results of the detected objects(bounding box, class label and probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1.3.1</b> First we define some constants needed by our object detection algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 600\n",
    "scale = 1.5\n",
    "step = 16\n",
    "roi_size = (224,224)\n",
    "INPUT_SIZE = (224, 224)\n",
    "threshold_prob = 0.90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1.3.2 </b> We load our ResNet classification CNN and input images from local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(weights=\"imagenet\", include_top=True)\n",
    "\n",
    "input_folder = './raccoon_dataset-master/images/'\n",
    "image_list = load_images(input_folder)\n",
    "\n",
    "(H, W) = image_list[2].shape[:2] #We are taking the second image from out dataset of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1.3.3 </b> We initialize our image pyramid generator object and then create the scaled images from the pyramid and then preprocess the scaled images.<br> We also put the regions of interest (ROIs) generated from pyramid + sliding window output into roi_window list and store the (x, y)-coordinates of where the ROI was in the original image in the loc_in_image list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyramid = image_pyramid(image_list[2], scale=scale, minSize=roi_size)\n",
    "# initialize two lists, one to hold the ROIs generated from the image\n",
    "# pyramid and sliding window, and another list used to store the\n",
    "# (x, y)-coordinates of where the ROI was in the original image\n",
    "roi_window = []\n",
    "loc_in_image = []\n",
    "\n",
    "for image in pyramid:\n",
    "    scale = W / float(image.shape[1])\n",
    "    for (x, y, roi) in sliding_window(image, step, roi_size):\n",
    "        x = int(x * scale)\n",
    "        y = int(y * scale)\n",
    "        w = int(roi_size[0] * scale)\n",
    "        h = int(roi_size[1] * scale)\n",
    "        \n",
    "        roi = cv2.resize(roi, INPUT_SIZE)\n",
    "        roi = img_to_array(roi)\n",
    "        roi = preprocess_input(roi)\n",
    "        \n",
    "        roi_window.append(roi)\n",
    "        loc_in_image.append((x, y, x + w, y + h))\n",
    "        \n",
    "roi_window = np.array(roi_window, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1.3.4 </b>  Inputting the ROIs into our pre-trained ResNet image classifier.<br>\n",
    "\n",
    "After feeding the ROIs in the model , we decodes the predictions, grabbing only the top prediction for each ROI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "predictions = model.predict(roi_window)\n",
    "end = time.time()\n",
    "print(\"Time taken :\", end - start)\n",
    "\n",
    "predictions = imagenet_utils.decode_predictions(predictions, top=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1.3.5 </b> We loop over the predictions , and take ImageNet ID, class label, and probability and then check to see if the minimum confidence has been met <br>(i.e if the probability of the predicted ROIs is above the threshold probabilty defined in the constants earlier).<br>\n",
    "\n",
    "Then we update the labels dictionary(labels) with the bounding box and prob score tuple (box, probs) associated with each class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "\n",
    "for (i, p) in enumerate(predictions):\n",
    "    (imagenetID, label, probs) = p[0]\n",
    "    # filtering out weaker detections by checking the predicted probability\n",
    "    # is greater than the threshols probability\n",
    "    if probs >= threshold_prob:\n",
    "        box = loc_in_image[i]\n",
    "        labels_list = labels.get(label, [])\n",
    "        labels_list.append((box, probs))\n",
    "        labels[label] = labels_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 1.3.6 Non-Maxima Supression </b> <br>\n",
    "\n",
    "Since there are multiple overlapping bounding boxes detected for each class label,we use non-maxima suppression (NMS) which gives us the bounding boxes with higher confidence9among the overlapping ones).\n",
    "\n",
    "Before that we visualise the bounding box predictions in the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in labels.keys():\n",
    "    clone = image_list[2].copy()\n",
    "    for (box, prob) in labels[label]:\n",
    "        (startX, startY, endX, endY) = box\n",
    "        cv2.rectangle(clone, (startX, startY), (endX, endY),(0, 255, 0), 2)\n",
    "    cv2.imshow(\"Before\", clone)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    clone = image_list[2].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply NMS and then visualise the final bounding box predictions in the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in labels.keys():\n",
    "    boxes = np.array([p[0] for p in labels[label]])\n",
    "    # print(boxes)\n",
    "    prob = np.array([p[1] for p in labels[label]])\n",
    "    boxes = non_max_suppression(boxes, prob)\n",
    "    print(boxes)\n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "            cv2.rectangle(clone, (startX, startY), (endX, endY),(0, 255, 0), 2)\n",
    "            y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "            cv2.putText(clone, label, (startX, y),cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "    cv2.imshow(\"After\", clone)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    clone = image_list[2].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
